{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "997ccfeb",
   "metadata": {},
   "source": [
    "# Tumor Tracer\n",
    "## Codigo hecho por Aimé Moral y Gabriel Juan\n",
    "### El codigo esta separado en dos partes\n",
    "\n",
    "#### Parte 1: Machine Learning\n",
    "Tomaremos cada fotos, sin importar el paciente, extraeremos el contorno del cancer y compararemos con la respuesta del dataset\n",
    "\n",
    "#### Parte 2: Deep Learning\n",
    "Agruparemos las fotos por paciente, extraeremos los concornos de cada nivel, de forma ordenada. Una vez se tienen todo, se hara una nuve de puntos 3D donde se podra ver el centro del cluster y al ir comparando capa a capa, se podra ver si alguna capa no se ha detectado de una forma coherente con respecto al resto de capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2257f5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: opencv-python in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: pandas in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: scikit-learn in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: scikit-image in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: scipy in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (1.15.3)\n",
      "Requirement already satisfied: seaborn in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: tensorflow in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: keras in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (3.12.0)\n",
      "Requirement already satisfied: albumentations in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (2.0.8)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from scikit-image) (3.4.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from scikit-image) (2025.5.10)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from scikit-image) (2.37.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: setuptools in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (57.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (6.33.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: optree in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from keras) (0.18.0)\n",
      "Requirement already satisfied: namex in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: rich in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from keras) (14.2.0)\n",
      "Requirement already satisfied: PyYAML in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from albumentations) (6.0.3)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from albumentations) (2.12.5)\n",
      "Requirement already satisfied: albucore==0.0.24 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from albumentations) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from albumentations) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from albucore==0.0.24->albumentations) (4.4.0)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (2.41.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.4)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from rich->keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from rich->keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\UAB\\3º 1º\\APC\\APC_Cass_Kaggle\\Tumor-Tracer\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\UAB\\3º 1º\\APC\\APC_Cass_Kaggle\\Tumor-Tracer\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.7-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp310-cp310-win_amd64.whl (8.9 MB)\n",
      "Collecting scikit-image\n",
      "  Using cached scikit_image-0.25.2-cp310-cp310-win_amd64.whl (12.8 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp310-cp310-win_amd64.whl (331.7 MB)\n",
      "Collecting keras\n",
      "  Using cached keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "Collecting albumentations\n",
      "  Using cached albumentations-2.0.8-py3-none-any.whl (369 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting pillow>=8\n",
      "  Using cached pillow-12.0.0-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "Collecting pyparsing>=3\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.61.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting networkx>=3.0\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Collecting lazy-loader>=0.4\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Collecting tifffile>=2022.8.12\n",
      "  Using cached tifffile-2025.5.10-py3-none-any.whl (226 kB)\n",
      "Collecting imageio!=2.35.0,>=2.33\n",
      "  Using cached imageio-2.37.2-py3-none-any.whl (317 kB)\n",
      "Collecting flatbuffers>=24.3.25\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Collecting protobuf>=5.28.0\n",
      "  Using cached protobuf-6.33.1-cp310-abi3-win_amd64.whl (436 kB)\n",
      "Collecting opt_einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1\n",
      "  Using cached ml_dtypes-0.5.4-cp310-cp310-win_amd64.whl (210 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Using cached gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.76.0-cp310-cp310-win_amd64.whl (4.7 MB)\n",
      "Collecting tensorboard~=2.20.0\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting google_pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=3.11.0\n",
      "  Using cached h5py-3.15.1-cp310-cp310-win_amd64.whl (2.9 MB)\n",
      "Requirement already satisfied: setuptools in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (57.4.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-2.0.1-cp310-cp310-win_amd64.whl (60 kB)\n",
      "Collecting rich\n",
      "  Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Collecting optree\n",
      "  Using cached optree-0.18.0-cp310-cp310-win_amd64.whl (302 kB)\n",
      "Collecting namex\n",
      "  Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting albucore==0.0.24\n",
      "  Using cached albucore-0.0.24-py3-none-any.whl (15 kB)\n",
      "Collecting PyYAML\n",
      "  Using cached pyyaml-6.0.3-cp310-cp310-win_amd64.whl (158 kB)\n",
      "Collecting pydantic>=2.9.2\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Collecting opencv-python-headless>=4.9.0.80\n",
      "  Using cached opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl (38.9 MB)\n",
      "Collecting simsimd>=5.9.2\n",
      "  Using cached simsimd-6.5.3-cp310-cp310-win_amd64.whl (94 kB)\n",
      "Collecting stringzilla>=3.10.4\n",
      "  Using cached stringzilla-4.4.0-cp310-cp310-win_amd64.whl (130 kB)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Collecting pydantic-core==2.41.5\n",
      "  Using cached pydantic_core-2.41.5-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "Collecting typing-inspection>=0.4.2\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.4-cp310-cp310-win_amd64.whl (107 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
      "Collecting markupsafe>=2.1.1\n",
      "  Using cached markupsafe-3.0.3-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\uab\\3º 1º\\apc\\apc_cass_kaggle\\tumor-tracer\\venv\\lib\\site-packages (from rich->keras) (2.19.2)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: mdurl, numpy, markupsafe, markdown-it-py, wheel, werkzeug, urllib3, tzdata, typing-inspection, tensorboard-data-server, stringzilla, simsimd, rich, pytz, pyparsing, pydantic-core, protobuf, pillow, optree, opencv-python-headless, namex, ml-dtypes, markdown, kiwisolver, idna, h5py, grpcio, fonttools, cycler, contourpy, charset-normalizer, certifi, annotated-types, absl-py, wrapt, tifffile, threadpoolctl, termcolor, tensorboard, scipy, requests, PyYAML, pydantic, pandas, opt-einsum, networkx, matplotlib, libclang, lazy-loader, keras, joblib, imageio, google-pasta, gast, flatbuffers, astunparse, albucore, tensorflow, seaborn, scikit-learn, scikit-image, opencv-python, albumentations\n",
      "Successfully installed PyYAML-6.0.3 absl-py-2.3.1 albucore-0.0.24 albumentations-2.0.8 annotated-types-0.7.0 astunparse-1.6.3 certifi-2025.11.12 charset-normalizer-3.4.4 contourpy-1.3.2 cycler-0.12.1 flatbuffers-25.9.23 fonttools-4.61.0 gast-0.7.0 google-pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 idna-3.11 imageio-2.37.2 joblib-1.5.2 keras-3.12.0 kiwisolver-1.4.9 lazy-loader-0.4 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 markupsafe-3.0.3 matplotlib-3.10.7 mdurl-0.1.2 ml-dtypes-0.5.4 namex-0.1.0 networkx-3.4.2 numpy-2.2.6 opencv-python-4.12.0.88 opencv-python-headless-4.12.0.88 opt-einsum-3.4.0 optree-0.18.0 pandas-2.3.3 pillow-12.0.0 protobuf-6.33.1 pydantic-2.12.5 pydantic-core-2.41.5 pyparsing-3.2.5 pytz-2025.2 requests-2.32.5 rich-14.2.0 scikit-image-0.25.2 scikit-learn-1.7.2 scipy-1.15.3 seaborn-0.13.2 simsimd-6.5.3 stringzilla-4.4.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 threadpoolctl-3.6.0 tifffile-2025.5.10 typing-inspection-0.4.2 tzdata-2025.2 urllib3-2.5.0 werkzeug-3.1.4 wheel-0.45.1 wrapt-2.0.1\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy opencv-python pandas matplotlib scikit-learn scikit-image scipy seaborn tensorflow keras albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3042b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import jaccard_score\n",
    "from skimage.filters import roberts, sobel, scharr, prewitt\n",
    "from scipy import ndimage as nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a41b9fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando migración de archivos ---\n",
      "--- Proceso terminado ---\n",
      "Se han copiado 7858 imágenes a '../data/dataset_plano/'\n",
      "--- Proceso terminado ---\n",
      "Se han copiado 7858 imágenes a '../data/dataset_plano/'\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "# Redefinicion de las carpetas\n",
    "# Rutas de configuración\n",
    "ORIGINAL_DATASET_PATH = \"../data/kaggle_3m/\"  # Donde están las carpetas ahora\n",
    "NEW_FLAT_PATH = \"../data/dataset_plano/\"      # Donde las quieres poner\n",
    "\n",
    "# 1. Crear la carpeta nueva si no existe\n",
    "if not os.path.exists(NEW_FLAT_PATH):\n",
    "    os.makedirs(NEW_FLAT_PATH)\n",
    "    print(f\"Carpeta creada: {NEW_FLAT_PATH}\")\n",
    "\n",
    "print(\"--- Iniciando migración de archivos ---\")\n",
    "\n",
    "# 2. Buscar todos los archivos .tif en subcarpetas\n",
    "# En Windows usamos '**/*.tif' con recursive=True\n",
    "files = glob.glob(os.path.join(ORIGINAL_DATASET_PATH, '**', '*.tif'), recursive=True)\n",
    "\n",
    "count = 0\n",
    "for file_path in files:\n",
    "    # Obtener solo el nombre del archivo (ej: TCGA_CS_4941_19960909_1.tif)\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    # Definir la nueva ruta de destino\n",
    "    dst_path = os.path.join(NEW_FLAT_PATH, filename)\n",
    "    \n",
    "    # Evitar sobrescribir si ya existe\n",
    "    if not os.path.exists(dst_path):\n",
    "        # Copiar el archivo (usamos copy2 para preservar metadatos)\n",
    "        shutil.copy2(file_path, dst_path)\n",
    "        count += 1\n",
    "\n",
    "print(f\"--- Proceso terminado ---\")\n",
    "print(f\"Se han copiado {count} imágenes a '{NEW_FLAT_PATH}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345c356",
   "metadata": {},
   "source": [
    "## Configuracion y Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e99cfb71",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Ajusta esta ruta a donde tengas descomprimido tu dataset\n",
    "DATASET_PATH = \"../data/dataset_plano/\" \n",
    "\n",
    "def extract_features(img):\n",
    "    \"\"\"\n",
    "    Esta función toma una imagen y crea un DataFrame donde cada fila\n",
    "    es un píxel y cada columna es una característica (feature) extraída\n",
    "    usando filtros clásicos.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Imagen original (1D array)\n",
    "    img2 = img.reshape(-1)\n",
    "    df['Original Image'] = img2\n",
    "\n",
    "    # Feature 1: Gabor Filter (Textura)\n",
    "    # Generamos varios filtros Gabor con diferentes orientaciones/tamaños\n",
    "    num = 1\n",
    "    kernels = []\n",
    "    for theta in range(2):\n",
    "        theta = theta / 4. * np.pi\n",
    "        for sigma in (1, 3):\n",
    "            lamda = np.pi / 4\n",
    "            gamma = 0.5\n",
    "            ksize = 9\n",
    "            kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lamda, gamma, 0, ktype=cv2.CV_32F)\n",
    "            fimg = cv2.filter2D(img, cv2.CV_8UC3, kernel)\n",
    "            filtered_img = fimg.reshape(-1)\n",
    "            df[f'Gabor_{num}'] = filtered_img\n",
    "            num += 1\n",
    "\n",
    "    # Feature 2: Canny Edge (Bordes)\n",
    "    edges = cv2.Canny(img, 100, 200)\n",
    "    df['Canny Edge'] = edges.reshape(-1)\n",
    "\n",
    "    # Feature 3: Roberts Edge\n",
    "    edge_roberts = roberts(img)\n",
    "    df['Roberts'] = edge_roberts.reshape(-1)\n",
    "\n",
    "    # Feature 4: Sobel\n",
    "    edge_sobel = sobel(img)\n",
    "    df['Sobel'] = edge_sobel.reshape(-1)\n",
    "\n",
    "    # Feature 5: Gaussian Blur (para suavizado/vecindad)\n",
    "    gaussian_img = nd.gaussian_filter(img, sigma=3)\n",
    "    df['Gaussian'] = gaussian_img.reshape(-1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_contours(mask):\n",
    "    \"\"\"\n",
    "    Obtiene los contornos de una máscara binaria.\n",
    "    \"\"\"\n",
    "    # Asegurarse de que sea uint8\n",
    "    mask = mask.astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return contours\n",
    "\n",
    "def calculate_iou(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula Intersection over Union (Jaccard Score).\n",
    "    Es la métrica estándar para segmentación.\n",
    "    \"\"\"\n",
    "    # Aplanar para comparar\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    # Usamos average='binary' asumiendo 0 fondo, 1 tumor\n",
    "    return jaccard_score(y_true, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d8b6d1",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO DEL MODELO (Pequeño ejemplo)\n",
    "Para ML Clásico, no podemos entrenar con TODAS las fotos a la vez en RAM fácilmente. Seleccionamos algunas imágenes representativas para entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2033d7f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Buscando imágenes ---\n",
      "Encontradas 3929 imágenes con sus máscaras correspondientes\n",
      "--- Extrayendo características y entrenando Random Forest con 50 imágenes ---\n",
      "Size of training subset: 50 images -- Size of masks: 50\n",
      "Encontradas 3929 imágenes con sus máscaras correspondientes\n",
      "--- Extrayendo características y entrenando Random Forest con 50 imágenes ---\n",
      "Size of training subset: 50 images -- Size of masks: 50\n",
      "--- Modelo Entrenado Exitosamente ---\n",
      "--- Modelo Entrenado Exitosamente ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Buscando imágenes ---\")\n",
    "# Patrón para encontrar imágenes y máscaras\n",
    "# Buscamos todas las máscaras primero y derivamos las imágenes originales\n",
    "all_tif_files = glob.glob(os.path.join(DATASET_PATH, '*.tif'))\n",
    "\n",
    "# Separamos: máscaras terminan en _mask.tif, imágenes NO terminan en _mask.tif\n",
    "mask_paths = [f for f in all_tif_files if f.endswith('_mask.tif')]\n",
    "image_paths = [f for f in all_tif_files if not f.endswith('_mask.tif')]\n",
    "\n",
    "# Filtramos solo los pares donde ambos archivos existen\n",
    "valid_pairs = [(img, mask) for img, mask in zip(image_paths, mask_paths) if os.path.exists(img)]\n",
    "image_paths = [p[0] for p in valid_pairs]\n",
    "mask_paths = [p[1] for p in valid_pairs]\n",
    "\n",
    "print(f\"Encontradas {len(image_paths)} imágenes con sus máscaras correspondientes\")\n",
    "\n",
    "# Tomamos una muestra para entrenar (ej. 100 imágenes) para que corra rápido el ejemplo\n",
    "# En producción, usa más datos y balancea (mismos pixeles tumor que no-tumor)\n",
    "subset_size = 50 \n",
    "train_imgs = image_paths[:subset_size]\n",
    "train_masks = mask_paths[:subset_size]\n",
    "\n",
    "print(f\"--- Extrayendo características y entrenando Random Forest con {subset_size} imágenes ---\")\n",
    "\n",
    "X_list = []\n",
    "Y_list = []\n",
    "print(f\"Size of training subset: {len(train_imgs)} images -- Size of masks: {len(train_masks)}\")\n",
    "for img_path, mask_path in zip(train_imgs, train_masks):\n",
    "    if not os.path.exists(mask_path): continue\n",
    "    \n",
    "    # Lectura en escala de grises\n",
    "    img = cv2.imread(img_path, 0) \n",
    "    mask = cv2.imread(mask_path, 0)\n",
    "    \n",
    "    # Normalizar máscara a 0 y 1\n",
    "    mask = mask // 255 \n",
    "\n",
    "    # Solo entrenamos si la imagen tiene tumor (para que el modelo aprenda qué es un tumor)\n",
    "    if np.max(mask) == 1:\n",
    "        features = extract_features(img)\n",
    "        X_list.append(features)\n",
    "        Y_list.append(mask.reshape(-1))\n",
    "\n",
    "if len(X_list) == 0:\n",
    "    print(\"Error: No se encontraron máscaras con tumores en el subset.\")\n",
    "    exit()\n",
    "\n",
    "# Concatenar todos los datos\n",
    "X = pd.concat(X_list)\n",
    "Y = np.concatenate(Y_list)\n",
    "\n",
    "# Entrenar Random Forest\n",
    "# n_estimators=50 para velocidad, súbelo a 100 o 200 para mejor precisión\n",
    "model = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)\n",
    "model.fit(X, Y)\n",
    "\n",
    "print(\"--- Modelo Entrenado Exitosamente ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a27e6a",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a98c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PARTE 2: AGRUPACIÓN POR PACIENTE Y ANÁLISIS\n",
    "# ==========================================\n",
    "\n",
    "print(\"--- Iniciando análisis por paciente (Parte 2) ---\")\n",
    "\n",
    "# Diccionario para guardar resultados: { 'Patient_ID': [ {datos_slice_1}, {datos_slice_2} ] }\n",
    "patients_data = {}\n",
    "\n",
    "# Usamos algunas imágenes de prueba que el modelo NO haya visto\n",
    "test_imgs = image_paths[subset_size:subset_size+10] \n",
    "test_masks = mask_paths[subset_size:subset_size+10]\n",
    "\n",
    "for img_path, mask_path in zip(test_imgs, test_masks):\n",
    "    if not os.path.exists(mask_path): continue\n",
    "\n",
    "    # 1. Obtener ID del paciente desde la ruta\n",
    "    # Estructura típica: .../TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_11.tif\n",
    "    # El ID del paciente suele ser la carpeta padre\n",
    "    patient_id = os.path.basename(os.path.dirname(img_path))\n",
    "    filename = os.path.basename(img_path)\n",
    "\n",
    "    # 2. Cargar imagen y verdad terreno (Ground Truth)\n",
    "    img = cv2.imread(img_path, 0)\n",
    "    gt_mask = cv2.imread(mask_path, 0) // 255\n",
    "\n",
    "    # 3. Predecir con el modelo de ML Clásico\n",
    "    df_features = extract_features(img)\n",
    "    prediction_flat = model.predict(df_features)\n",
    "    prediction_mask = prediction_flat.reshape(img.shape)\n",
    "\n",
    "    # 4. Limpieza (Morfología matemática)\n",
    "    # Importante en ML clásico para quitar ruido \"pimienta\"\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    prediction_cleaned = cv2.morphologyEx(prediction_mask.astype(np.uint8), cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # 5. Obtener Contornos\n",
    "    # Contorno Predicho\n",
    "    pred_contours = get_contours(prediction_cleaned)\n",
    "    # Contorno Real (Ground Truth)\n",
    "    gt_contours = get_contours(gt_mask)\n",
    "\n",
    "    # 6. Comparación (IoU)\n",
    "    iou = calculate_iou(gt_mask, prediction_cleaned)\n",
    "\n",
    "    # 7. Guardar en estructura de datos\n",
    "    if patient_id not in patients_data:\n",
    "        patients_data[patient_id] = []\n",
    "\n",
    "    patients_data[patient_id].append({\n",
    "        'filename': filename,\n",
    "        'iou': iou,\n",
    "        'has_tumor_pred': 1 if np.sum(prediction_cleaned) > 0 else 0,\n",
    "        'has_tumor_real': 1 if np.sum(gt_mask) > 0 else 0,\n",
    "        'pred_contours_count': len(pred_contours)\n",
    "    })\n",
    "\n",
    "    # Visualización de un ejemplo (Opcional)\n",
    "    if iou > 0.1 and iou < 0.9: # Mostrar casos interesantes\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(f\"MRI Original\\nPac: {patient_id}\")\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Predicción (ML Clásico)\")\n",
    "        plt.imshow(prediction_cleaned, cmap='gray')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(f\"Realidad (Ground Truth)\\nIoU: {iou:.2f}\")\n",
    "        plt.imshow(gt_mask, cmap='gray')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        # Solo mostramos uno y rompemos el bucle de visualización para no llenar la pantalla\n",
    "        # break \n",
    "\n",
    "# ==========================================\n",
    "# RESUMEN POR PACIENTE\n",
    "# ==========================================\n",
    "print(\"\\n--- Resultados Agrupados por Paciente ---\")\n",
    "for pid, slices in patients_data.items():\n",
    "    avg_iou = np.mean([s['iou'] for s in slices])\n",
    "    tumor_slices_detected = sum([s['has_tumor_pred'] for s in slices])\n",
    "    print(f\"Paciente: {pid} | Promedio IoU: {avg_iou:.4f} | Slices con tumor detectados: {tumor_slices_detected}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tumor-tracer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
